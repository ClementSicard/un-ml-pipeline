{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unml.modules.summarize import Summarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "summarizer = Summarizer()\n",
    "tokenizer = summarizer.summarizer.model.tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (19000 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Detect the end of a sentence\n",
    "def is_eos(token: str) -> bool:\n",
    "    \"\"\"\n",
    "    Detect if a token is the end of a sentence\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    `token` : `str`\n",
    "        The token to be checked\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    `bool`\n",
    "        True if the token is the end of a sentence, False otherwise\n",
    "    \"\"\"\n",
    "    return token in [\".\", \"!\", \"?\"]\n",
    "\n",
    "\n",
    "\n",
    "tokens = summarizer.summarizer.model.tokenizer.tokenize(\"Hello world! Je suis Clément et j'aime bien le foot.\" * 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from typing import List\n",
    "\n",
    "def chunk_tokens(tokens: List[str], max_chunk_size: int = 1024) -> List[str]:\n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "    current_sentence = []\n",
    "    \n",
    "    for token in tokens:\n",
    "        print(\"=\" * 20)\n",
    "        print(f'Token: {token}')\n",
    "        print(f'Current sentence: {current_sentence}')\n",
    "        print(\"=\" * 20)\n",
    "        current_sentence += [token]\n",
    "        is_last = token == tokens[-1]\n",
    "        \n",
    "        if is_eos(token) or is_last:\n",
    "            print(f'EOS: {token}')\n",
    "            print(f'Condition: {len(current_chunk)} + {len(current_sentence)} <= {max_chunk_size} : {len(current_chunk) + len(current_sentence) <= max_chunk_size}')\n",
    "            if len(current_chunk) + len(current_sentence) <= max_chunk_size:\n",
    "                current_chunk.extend(current_sentence)\n",
    "                current_sentence = []\n",
    "            else:\n",
    "                chunks.append(current_chunk)\n",
    "                current_chunk = current_sentence.copy()\n",
    "                current_sentence = []\n",
    "        print(f'Chunks: {chunks}')\n",
    "        print(f'Current chunk: {current_chunk}')\n",
    "        print()\n",
    "\n",
    "    chunks.append(current_chunk)\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================\n",
      "Token: Hello\n",
      "Current sentence: []\n",
      "====================\n",
      "Chunks: []\n",
      "Current chunk: []\n",
      "\n",
      "====================\n",
      "Token: Ġworld\n",
      "Current sentence: ['Hello']\n",
      "====================\n",
      "Chunks: []\n",
      "Current chunk: []\n",
      "\n",
      "====================\n",
      "Token: !\n",
      "Current sentence: ['Hello', 'Ġworld']\n",
      "====================\n",
      "EOS: !\n",
      "Condition: 0 + 3 <= 14 : True\n",
      "Chunks: []\n",
      "Current chunk: ['Hello', 'Ġworld', '!']\n",
      "\n",
      "====================\n",
      "Token: ĠI\n",
      "Current sentence: []\n",
      "====================\n",
      "Chunks: []\n",
      "Current chunk: ['Hello', 'Ġworld', '!']\n",
      "\n",
      "====================\n",
      "Token: Ġam\n",
      "Current sentence: ['ĠI']\n",
      "====================\n",
      "Chunks: []\n",
      "Current chunk: ['Hello', 'Ġworld', '!']\n",
      "\n",
      "====================\n",
      "Token: ĠCl\n",
      "Current sentence: ['ĠI', 'Ġam']\n",
      "====================\n",
      "Chunks: []\n",
      "Current chunk: ['Hello', 'Ġworld', '!']\n",
      "\n",
      "====================\n",
      "Token: Ã©\n",
      "Current sentence: ['ĠI', 'Ġam', 'ĠCl']\n",
      "====================\n",
      "Chunks: []\n",
      "Current chunk: ['Hello', 'Ġworld', '!']\n",
      "\n",
      "====================\n",
      "Token: ment\n",
      "Current sentence: ['ĠI', 'Ġam', 'ĠCl', 'Ã©']\n",
      "====================\n",
      "Chunks: []\n",
      "Current chunk: ['Hello', 'Ġworld', '!']\n",
      "\n",
      "====================\n",
      "Token: Ġand\n",
      "Current sentence: ['ĠI', 'Ġam', 'ĠCl', 'Ã©', 'ment']\n",
      "====================\n",
      "Chunks: []\n",
      "Current chunk: ['Hello', 'Ġworld', '!']\n",
      "\n",
      "====================\n",
      "Token: ĠI\n",
      "Current sentence: ['ĠI', 'Ġam', 'ĠCl', 'Ã©', 'ment', 'Ġand']\n",
      "====================\n",
      "Chunks: []\n",
      "Current chunk: ['Hello', 'Ġworld', '!']\n",
      "\n",
      "====================\n",
      "Token: Ġlike\n",
      "Current sentence: ['ĠI', 'Ġam', 'ĠCl', 'Ã©', 'ment', 'Ġand', 'ĠI']\n",
      "====================\n",
      "Chunks: []\n",
      "Current chunk: ['Hello', 'Ġworld', '!']\n",
      "\n",
      "====================\n",
      "Token: Ġfootball\n",
      "Current sentence: ['ĠI', 'Ġam', 'ĠCl', 'Ã©', 'ment', 'Ġand', 'ĠI', 'Ġlike']\n",
      "====================\n",
      "Chunks: []\n",
      "Current chunk: ['Hello', 'Ġworld', '!']\n",
      "\n",
      "====================\n",
      "Token: .\n",
      "Current sentence: ['ĠI', 'Ġam', 'ĠCl', 'Ã©', 'ment', 'Ġand', 'ĠI', 'Ġlike', 'Ġfootball']\n",
      "====================\n",
      "EOS: .\n",
      "Condition: 3 + 10 <= 14 : True\n",
      "Chunks: []\n",
      "Current chunk: ['Hello', 'Ġworld', '!', 'ĠI', 'Ġam', 'ĠCl', 'Ã©', 'ment', 'Ġand', 'ĠI', 'Ġlike', 'Ġfootball', '.']\n",
      "\n",
      "====================\n",
      "Token: ĠI\n",
      "Current sentence: []\n",
      "====================\n",
      "Chunks: []\n",
      "Current chunk: ['Hello', 'Ġworld', '!', 'ĠI', 'Ġam', 'ĠCl', 'Ã©', 'ment', 'Ġand', 'ĠI', 'Ġlike', 'Ġfootball', '.']\n",
      "\n",
      "====================\n",
      "Token: Ġlove\n",
      "Current sentence: ['ĠI']\n",
      "====================\n",
      "Chunks: []\n",
      "Current chunk: ['Hello', 'Ġworld', '!', 'ĠI', 'Ġam', 'ĠCl', 'Ã©', 'ment', 'Ġand', 'ĠI', 'Ġlike', 'Ġfootball', '.']\n",
      "\n",
      "====================\n",
      "Token: Ġeating\n",
      "Current sentence: ['ĠI', 'Ġlove']\n",
      "====================\n",
      "Chunks: []\n",
      "Current chunk: ['Hello', 'Ġworld', '!', 'ĠI', 'Ġam', 'ĠCl', 'Ã©', 'ment', 'Ġand', 'ĠI', 'Ġlike', 'Ġfootball', '.']\n",
      "\n",
      "====================\n",
      "Token: Ġice\n",
      "Current sentence: ['ĠI', 'Ġlove', 'Ġeating']\n",
      "====================\n",
      "Chunks: []\n",
      "Current chunk: ['Hello', 'Ġworld', '!', 'ĠI', 'Ġam', 'ĠCl', 'Ã©', 'ment', 'Ġand', 'ĠI', 'Ġlike', 'Ġfootball', '.']\n",
      "\n",
      "====================\n",
      "Token: -\n",
      "Current sentence: ['ĠI', 'Ġlove', 'Ġeating', 'Ġice']\n",
      "====================\n",
      "Chunks: []\n",
      "Current chunk: ['Hello', 'Ġworld', '!', 'ĠI', 'Ġam', 'ĠCl', 'Ã©', 'ment', 'Ġand', 'ĠI', 'Ġlike', 'Ġfootball', '.']\n",
      "\n",
      "====================\n",
      "Token: cream\n",
      "Current sentence: ['ĠI', 'Ġlove', 'Ġeating', 'Ġice', '-']\n",
      "====================\n",
      "Chunks: []\n",
      "Current chunk: ['Hello', 'Ġworld', '!', 'ĠI', 'Ġam', 'ĠCl', 'Ã©', 'ment', 'Ġand', 'ĠI', 'Ġlike', 'Ġfootball', '.']\n",
      "\n",
      "====================\n",
      "Token: ,\n",
      "Current sentence: ['ĠI', 'Ġlove', 'Ġeating', 'Ġice', '-', 'cream']\n",
      "====================\n",
      "Chunks: []\n",
      "Current chunk: ['Hello', 'Ġworld', '!', 'ĠI', 'Ġam', 'ĠCl', 'Ã©', 'ment', 'Ġand', 'ĠI', 'Ġlike', 'Ġfootball', '.']\n",
      "\n",
      "====================\n",
      "Token: Ġand\n",
      "Current sentence: ['ĠI', 'Ġlove', 'Ġeating', 'Ġice', '-', 'cream', ',']\n",
      "====================\n",
      "Chunks: []\n",
      "Current chunk: ['Hello', 'Ġworld', '!', 'ĠI', 'Ġam', 'ĠCl', 'Ã©', 'ment', 'Ġand', 'ĠI', 'Ġlike', 'Ġfootball', '.']\n",
      "\n",
      "====================\n",
      "Token: Ġsometimes\n",
      "Current sentence: ['ĠI', 'Ġlove', 'Ġeating', 'Ġice', '-', 'cream', ',', 'Ġand']\n",
      "====================\n",
      "Chunks: []\n",
      "Current chunk: ['Hello', 'Ġworld', '!', 'ĠI', 'Ġam', 'ĠCl', 'Ã©', 'ment', 'Ġand', 'ĠI', 'Ġlike', 'Ġfootball', '.']\n",
      "\n",
      "====================\n",
      "Token: Ġpizza\n",
      "Current sentence: ['ĠI', 'Ġlove', 'Ġeating', 'Ġice', '-', 'cream', ',', 'Ġand', 'Ġsometimes']\n",
      "====================\n",
      "EOS: Ġpizza\n",
      "Condition: 13 + 10 <= 14 : False\n",
      "Chunks: [['Hello', 'Ġworld', '!', 'ĠI', 'Ġam', 'ĠCl', 'Ã©', 'ment', 'Ġand', 'ĠI', 'Ġlike', 'Ġfootball', '.']]\n",
      "Current chunk: ['ĠI', 'Ġlove', 'Ġeating', 'Ġice', '-', 'cream', ',', 'Ġand', 'Ġsometimes', 'Ġpizza']\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['Hello',\n",
       "  'Ġworld',\n",
       "  '!',\n",
       "  'ĠI',\n",
       "  'Ġam',\n",
       "  'ĠCl',\n",
       "  'Ã©',\n",
       "  'ment',\n",
       "  'Ġand',\n",
       "  'ĠI',\n",
       "  'Ġlike',\n",
       "  'Ġfootball',\n",
       "  '.'],\n",
       " ['ĠI',\n",
       "  'Ġlove',\n",
       "  'Ġeating',\n",
       "  'Ġice',\n",
       "  '-',\n",
       "  'cream',\n",
       "  ',',\n",
       "  'Ġand',\n",
       "  'Ġsometimes',\n",
       "  'Ġpizza']]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = tokenizer.tokenize(\"Hello world! I am Clément and I like football. I love eating ice-cream, and sometimes pizza\")\n",
    "chunk_tokens(tokens, max_chunk_size=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0', 'Ċ', '1', 'Ċ', '2']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize('0\\n1\\n2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "un-ml-pipeline-5nojIqN6-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
